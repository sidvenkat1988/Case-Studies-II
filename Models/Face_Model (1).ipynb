{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Face Model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGXUNe9FEkHW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "111bbd02-7eea-4021-aa3b-807c9feb45f5"
      },
      "source": [
        "%cd drive/My\\ Drive"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aELXsU2wEtMR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e46461f9-1361-4699-ddcf-129c2c35d547"
      },
      "source": [
        "%cd case\\ studies\\ 2"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/case studies 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZfO1BqLEu9e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d23ee7d4-367b-41a2-d8f2-0df0c00552fe"
      },
      "source": [
        "%cd Models/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/case studies 2/Models\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DES5j8bEwbG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e3f39be7-2266-4133-833d-371a0acf9e67"
      },
      "source": [
        "%cd Face\\ Models"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/case studies 2/Models/Face Models\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRA-JXKlExyf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "986d2736-51fe-4148-da3a-8d44b1a8f344"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "face_test_data.npy   face_train_data.npy   Trial\n",
            "face_test_label.npy  face_train_label.npy  Untitled\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtMongNUEyY_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "88dd9a96-371c-4219-8cc2-2ea7901f7d79"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Dropout, Flatten, BatchNormalization, Activation\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import cv2\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHkXl7ldE28y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = np.load('face_train_data.npy', allow_pickle=True)\n",
        "train_labels = np.load('face_train_label.npy', allow_pickle = True)\n",
        "test_data = np.load('face_test_data.npy', allow_pickle = True)\n",
        "test_labels = np.load('face_test_label.npy', allow_pickle = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWvFSwa3FFeP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df = pd.DataFrame(columns = ['train_labels'])\n",
        "train_df['train_labels'] = train_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JfooxtzFKXG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_df = pd.DataFrame(columns = ['test_labels'])\n",
        "test_df['test_labels'] = test_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suGVJgSwFMv2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = train_data/255.0\n",
        "X_test = test_data/255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eH2bo1KFOlg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = train_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmU4sSMvFQd_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_test = test_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRazd2a_FSQF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lb = LabelEncoder()\n",
        "y = to_categorical(lb.fit_transform(y))\n",
        "y_test = to_categorical(lb.fit_transform(y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZyuEXc7FURe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X[:24000]\n",
        "y_train = y[:24000]\n",
        "X_val = X[24000:]\n",
        "y_val = y[24000:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axXP2N5_FWCH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "664bcf9f-8174-4c81-ae3b-ca74580f9dc5"
      },
      "source": [
        "input_img = Input(shape = (64, 64, 3))\n",
        "x = Conv2D(32, (3,3), activation = 'relu', padding = 'same')(input_img)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(32, (3,3), activation = 'relu', padding = 'same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2,2), padding = 'same')(x)\n",
        "\n",
        "x = Conv2D(64, (3,3), activation = 'relu', padding = 'same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(64, (3,3), activation = 'relu', padding = 'same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2,2), padding = 'same')(x)\n",
        "\n",
        "x = Conv2D(128, (3,3), activation = 'relu', padding = 'same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(128, (3,3), activation = 'relu', padding = 'same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2,2), padding = 'same')(x)\n",
        "\n",
        "x = Conv2D(256, (3,3), activation = 'relu', padding = 'same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(256, (3,3), activation = 'relu', padding = 'same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2,2), padding = 'same')(x)\n",
        "\n",
        "x = Conv2D(512, (3,3), activation = 'relu', padding = 'same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(512, (3,3), activation = 'relu', padding = 'same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "encoded = MaxPooling2D((2,2), padding = 'same')(x)\n",
        "\n",
        "x = Conv2D(512, (3,3), activation = 'relu', padding = 'same')(encoded)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(512, (3,3), activation = 'relu', padding = 'same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = UpSampling2D((2,2))(x)\n",
        "\n",
        "x = Conv2D(256, (3,3), activation = 'relu', padding = 'same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(256, (3,3), activation = 'relu', padding = 'same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = UpSampling2D((2,2))(x)\n",
        "\n",
        "x = Conv2D(128, (3,3), activation = 'relu', padding = 'same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(128, (3,3), activation = 'relu', padding = 'same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = UpSampling2D((2,2))(x)\n",
        "\n",
        "x = Conv2D(64, (3,3), activation = 'relu', padding = 'same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(64, (3,3), activation = 'relu', padding = 'same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = UpSampling2D((2,2))(x)\n",
        "\n",
        "x = Conv2D(32, (3,3), activation = 'relu', padding = 'same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(32, (3,3), activation = 'relu', padding = 'same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = UpSampling2D((2,2))(x)\n",
        "\n",
        "decoded = Conv2D(3, (3,3), activation = 'softmax', padding = 'same')(x)\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2239: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9toe5US2FXyW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "4d9108a9-8b13-4733-e577-f0910d49a97e"
      },
      "source": [
        "autoencoder = Model(input_img, decoded)\n",
        "autoencoder.compile(loss = 'categorical_crossentropy', metrics = ['accuracy'], optimizer = 'adam')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpzOdfBFFaeK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5052a579-fc7b-4bfe-8167-01cf8241176e"
      },
      "source": [
        "autoencoder.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 64, 64, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 64, 64, 32)        896       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 64, 64, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 64, 64, 32)        9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 64, 64, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 32, 32, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 32, 32, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 32, 32, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 32, 32, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 16, 16, 128)       73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 16, 16, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 16, 16, 128)       147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 16, 16, 128)       512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 8, 8, 256)         295168    \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 8, 8, 256)         1024      \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 8, 8, 256)         590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 8, 8, 256)         1024      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 4, 4, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 4, 4, 512)         2048      \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 4, 4, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 4, 4, 512)         2048      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 2, 2, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 2, 2, 512)         2048      \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 2, 2, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 2, 2, 512)         2048      \n",
            "_________________________________________________________________\n",
            "up_sampling2d_1 (UpSampling2 (None, 4, 4, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 4, 4, 256)         1179904   \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 4, 4, 256)         1024      \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 4, 4, 256)         590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 4, 4, 256)         1024      \n",
            "_________________________________________________________________\n",
            "up_sampling2d_2 (UpSampling2 (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 8, 8, 128)         295040    \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 8, 8, 128)         147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "up_sampling2d_3 (UpSampling2 (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 16, 16, 64)        73792     \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 16, 16, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "up_sampling2d_4 (UpSampling2 (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 32, 32, 32)        18464     \n",
            "_________________________________________________________________\n",
            "batch_normalization_19 (Batc (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 32, 32, 32)        9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_20 (Batc (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "up_sampling2d_5 (UpSampling2 (None, 64, 64, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_21 (Conv2D)           (None, 64, 64, 3)         867       \n",
            "=================================================================\n",
            "Total params: 11,799,619\n",
            "Trainable params: 11,791,683\n",
            "Non-trainable params: 7,936\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krEbAshSFcjm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filepath = 'face_autoencoder.h5'\n",
        "checkpoint = ModelCheckpoint(filepath, monitor = 'val_acc', verbose = 1, save_best_only = True, mode = 'max')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pr9dG7TXFgkm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3855fe13-f7c6-41fa-e01f-5310bf1e6c97"
      },
      "source": [
        "autoencoder_train = autoencoder.fit(X_train, X_train, epochs = 100, batch_size = 256, verbose = 1, callbacks = callbacks_list, validation_data = [X_val, X_val])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Train on 24000 samples, validate on 7994 samples\n",
            "Epoch 1/100\n",
            "24000/24000 [==============================] - 53s 2ms/step - loss: 1.6020 - acc: 0.7999 - val_loss: 1.5914 - val_acc: 0.9083\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.90832, saving model to face_autoencoder.h5\n",
            "Epoch 2/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5703 - acc: 0.9107 - val_loss: 1.5792 - val_acc: 0.9326\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.90832 to 0.93261, saving model to face_autoencoder.h5\n",
            "Epoch 3/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5678 - acc: 0.9188 - val_loss: 1.5770 - val_acc: 0.9391\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.93261 to 0.93912, saving model to face_autoencoder.h5\n",
            "Epoch 4/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5666 - acc: 0.9228 - val_loss: 1.5745 - val_acc: 0.9379\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.93912\n",
            "Epoch 5/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5656 - acc: 0.9221 - val_loss: 1.5732 - val_acc: 0.9342\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.93912\n",
            "Epoch 6/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5646 - acc: 0.9210 - val_loss: 1.5728 - val_acc: 0.9258\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.93912\n",
            "Epoch 7/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5637 - acc: 0.9202 - val_loss: 1.5725 - val_acc: 0.9212\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.93912\n",
            "Epoch 8/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5627 - acc: 0.9203 - val_loss: 1.5710 - val_acc: 0.9103\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.93912\n",
            "Epoch 9/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5620 - acc: 0.9219 - val_loss: 1.5712 - val_acc: 0.8928\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.93912\n",
            "Epoch 10/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5612 - acc: 0.9236 - val_loss: 1.5698 - val_acc: 0.9107\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.93912\n",
            "Epoch 11/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5604 - acc: 0.9259 - val_loss: 1.5692 - val_acc: 0.9124\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.93912\n",
            "Epoch 12/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5599 - acc: 0.9281 - val_loss: 1.5701 - val_acc: 0.9171\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.93912\n",
            "Epoch 13/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5604 - acc: 0.9277 - val_loss: 1.5928 - val_acc: 0.8793\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.93912\n",
            "Epoch 14/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5591 - acc: 0.9312 - val_loss: 1.5710 - val_acc: 0.9066\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.93912\n",
            "Epoch 15/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5585 - acc: 0.9321 - val_loss: 1.5682 - val_acc: 0.9308\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.93912\n",
            "Epoch 16/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5590 - acc: 0.9322 - val_loss: 1.6562 - val_acc: 0.9006\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.93912\n",
            "Epoch 17/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5584 - acc: 0.9334 - val_loss: 1.5758 - val_acc: 0.9107\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.93912\n",
            "Epoch 18/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5596 - acc: 0.9320 - val_loss: 1.6884 - val_acc: 0.9001\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.93912\n",
            "Epoch 19/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5589 - acc: 0.9343 - val_loss: 1.5705 - val_acc: 0.9279\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.93912\n",
            "Epoch 20/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5580 - acc: 0.9358 - val_loss: 1.5672 - val_acc: 0.9183\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.93912\n",
            "Epoch 21/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5576 - acc: 0.9360 - val_loss: 1.5927 - val_acc: 0.9044\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.93912\n",
            "Epoch 22/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5574 - acc: 0.9360 - val_loss: 1.5671 - val_acc: 0.9249\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.93912\n",
            "Epoch 23/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5571 - acc: 0.9378 - val_loss: 1.5648 - val_acc: 0.9416\n",
            "\n",
            "Epoch 00023: val_acc improved from 0.93912 to 0.94160, saving model to face_autoencoder.h5\n",
            "Epoch 24/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5569 - acc: 0.9387 - val_loss: 1.5652 - val_acc: 0.9314\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.94160\n",
            "Epoch 25/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5567 - acc: 0.9407 - val_loss: 1.5649 - val_acc: 0.9297\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.94160\n",
            "Epoch 26/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5565 - acc: 0.9399 - val_loss: 1.5643 - val_acc: 0.9371\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.94160\n",
            "Epoch 27/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5564 - acc: 0.9413 - val_loss: 1.5651 - val_acc: 0.9210\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.94160\n",
            "Epoch 28/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5562 - acc: 0.9413 - val_loss: 1.5650 - val_acc: 0.9350\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.94160\n",
            "Epoch 29/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5564 - acc: 0.9412 - val_loss: 1.5643 - val_acc: 0.9335\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.94160\n",
            "Epoch 30/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5563 - acc: 0.9422 - val_loss: 1.6365 - val_acc: 0.8816\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.94160\n",
            "Epoch 31/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5566 - acc: 0.9417 - val_loss: 1.5647 - val_acc: 0.9305\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.94160\n",
            "Epoch 32/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5562 - acc: 0.9426 - val_loss: 1.5644 - val_acc: 0.9532\n",
            "\n",
            "Epoch 00032: val_acc improved from 0.94160 to 0.95322, saving model to face_autoencoder.h5\n",
            "Epoch 33/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5559 - acc: 0.9435 - val_loss: 1.5643 - val_acc: 0.9298\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.95322\n",
            "Epoch 34/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5557 - acc: 0.9437 - val_loss: 1.5639 - val_acc: 0.9386\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.95322\n",
            "Epoch 35/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5557 - acc: 0.9443 - val_loss: 1.5638 - val_acc: 0.9385\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.95322\n",
            "Epoch 36/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5589 - acc: 0.9371 - val_loss: 1.6979 - val_acc: 0.8499\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.95322\n",
            "Epoch 37/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5596 - acc: 0.9375 - val_loss: 1.6520 - val_acc: 0.5545\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.95322\n",
            "Epoch 38/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5577 - acc: 0.9406 - val_loss: 1.5773 - val_acc: 0.8497\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.95322\n",
            "Epoch 39/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5569 - acc: 0.9409 - val_loss: 1.5667 - val_acc: 0.9406\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.95322\n",
            "Epoch 40/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5564 - acc: 0.9424 - val_loss: 1.5648 - val_acc: 0.9437\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.95322\n",
            "Epoch 41/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5563 - acc: 0.9424 - val_loss: 1.5677 - val_acc: 0.9454\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.95322\n",
            "Epoch 42/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5562 - acc: 0.9428 - val_loss: 1.5666 - val_acc: 0.9250\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.95322\n",
            "Epoch 43/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5559 - acc: 0.9441 - val_loss: 1.5647 - val_acc: 0.9302\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.95322\n",
            "Epoch 44/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5560 - acc: 0.9443 - val_loss: 1.5650 - val_acc: 0.9315\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.95322\n",
            "Epoch 45/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5558 - acc: 0.9438 - val_loss: 1.5643 - val_acc: 0.9532\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.95322\n",
            "Epoch 46/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5555 - acc: 0.9455 - val_loss: 1.5646 - val_acc: 0.9537\n",
            "\n",
            "Epoch 00046: val_acc improved from 0.95322 to 0.95367, saving model to face_autoencoder.h5\n",
            "Epoch 47/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5555 - acc: 0.9449 - val_loss: 1.5636 - val_acc: 0.9506\n",
            "\n",
            "Epoch 00047: val_acc did not improve from 0.95367\n",
            "Epoch 48/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5554 - acc: 0.9464 - val_loss: 1.5638 - val_acc: 0.9467\n",
            "\n",
            "Epoch 00048: val_acc did not improve from 0.95367\n",
            "Epoch 49/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5553 - acc: 0.9463 - val_loss: 1.5628 - val_acc: 0.9420\n",
            "\n",
            "Epoch 00049: val_acc did not improve from 0.95367\n",
            "Epoch 50/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5552 - acc: 0.9461 - val_loss: 1.5636 - val_acc: 0.9511\n",
            "\n",
            "Epoch 00050: val_acc did not improve from 0.95367\n",
            "Epoch 51/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5553 - acc: 0.9461 - val_loss: 1.5636 - val_acc: 0.9581\n",
            "\n",
            "Epoch 00051: val_acc improved from 0.95367 to 0.95813, saving model to face_autoencoder.h5\n",
            "Epoch 52/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5555 - acc: 0.9455 - val_loss: 1.5649 - val_acc: 0.9548\n",
            "\n",
            "Epoch 00052: val_acc did not improve from 0.95813\n",
            "Epoch 53/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5552 - acc: 0.9472 - val_loss: 1.5636 - val_acc: 0.9480\n",
            "\n",
            "Epoch 00053: val_acc did not improve from 0.95813\n",
            "Epoch 54/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5551 - acc: 0.9473 - val_loss: 1.5629 - val_acc: 0.9384\n",
            "\n",
            "Epoch 00054: val_acc did not improve from 0.95813\n",
            "Epoch 55/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5561 - acc: 0.9448 - val_loss: 1.6136 - val_acc: 0.5904\n",
            "\n",
            "Epoch 00055: val_acc did not improve from 0.95813\n",
            "Epoch 56/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5564 - acc: 0.9439 - val_loss: 1.5738 - val_acc: 0.8547\n",
            "\n",
            "Epoch 00056: val_acc did not improve from 0.95813\n",
            "Epoch 57/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5554 - acc: 0.9459 - val_loss: 1.5650 - val_acc: 0.9214\n",
            "\n",
            "Epoch 00057: val_acc did not improve from 0.95813\n",
            "Epoch 58/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5552 - acc: 0.9475 - val_loss: 1.5635 - val_acc: 0.9445\n",
            "\n",
            "Epoch 00058: val_acc did not improve from 0.95813\n",
            "Epoch 59/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5551 - acc: 0.9473 - val_loss: 1.5635 - val_acc: 0.9342\n",
            "\n",
            "Epoch 00059: val_acc did not improve from 0.95813\n",
            "Epoch 60/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5550 - acc: 0.9479 - val_loss: 1.5643 - val_acc: 0.9413\n",
            "\n",
            "Epoch 00060: val_acc did not improve from 0.95813\n",
            "Epoch 61/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5550 - acc: 0.9483 - val_loss: 1.5628 - val_acc: 0.9307\n",
            "\n",
            "Epoch 00061: val_acc did not improve from 0.95813\n",
            "Epoch 62/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5550 - acc: 0.9484 - val_loss: 1.5629 - val_acc: 0.9637\n",
            "\n",
            "Epoch 00062: val_acc improved from 0.95813 to 0.96372, saving model to face_autoencoder.h5\n",
            "Epoch 63/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5548 - acc: 0.9484 - val_loss: 1.5632 - val_acc: 0.9334\n",
            "\n",
            "Epoch 00063: val_acc did not improve from 0.96372\n",
            "Epoch 64/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5548 - acc: 0.9484 - val_loss: 1.5629 - val_acc: 0.9577\n",
            "\n",
            "Epoch 00064: val_acc did not improve from 0.96372\n",
            "Epoch 65/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5548 - acc: 0.9492 - val_loss: 1.5627 - val_acc: 0.9467\n",
            "\n",
            "Epoch 00065: val_acc did not improve from 0.96372\n",
            "Epoch 66/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5547 - acc: 0.9484 - val_loss: 1.5625 - val_acc: 0.9616\n",
            "\n",
            "Epoch 00066: val_acc did not improve from 0.96372\n",
            "Epoch 67/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5562 - acc: 0.9456 - val_loss: 1.6100 - val_acc: 0.9137\n",
            "\n",
            "Epoch 00067: val_acc did not improve from 0.96372\n",
            "Epoch 68/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5554 - acc: 0.9471 - val_loss: 1.5747 - val_acc: 0.9225\n",
            "\n",
            "Epoch 00068: val_acc did not improve from 0.96372\n",
            "Epoch 69/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5552 - acc: 0.9470 - val_loss: 1.5651 - val_acc: 0.9486\n",
            "\n",
            "Epoch 00069: val_acc did not improve from 0.96372\n",
            "Epoch 70/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5549 - acc: 0.9492 - val_loss: 1.5646 - val_acc: 0.9325\n",
            "\n",
            "Epoch 00070: val_acc did not improve from 0.96372\n",
            "Epoch 71/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5548 - acc: 0.9484 - val_loss: 1.5630 - val_acc: 0.9596\n",
            "\n",
            "Epoch 00071: val_acc did not improve from 0.96372\n",
            "Epoch 72/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5547 - acc: 0.9492 - val_loss: 1.5624 - val_acc: 0.9312\n",
            "\n",
            "Epoch 00072: val_acc did not improve from 0.96372\n",
            "Epoch 73/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5547 - acc: 0.9487 - val_loss: 1.5636 - val_acc: 0.9634\n",
            "\n",
            "Epoch 00073: val_acc did not improve from 0.96372\n",
            "Epoch 74/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5546 - acc: 0.9498 - val_loss: 1.5625 - val_acc: 0.9604\n",
            "\n",
            "Epoch 00074: val_acc did not improve from 0.96372\n",
            "Epoch 75/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5545 - acc: 0.9498 - val_loss: 1.5624 - val_acc: 0.9432\n",
            "\n",
            "Epoch 00075: val_acc did not improve from 0.96372\n",
            "Epoch 76/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5545 - acc: 0.9496 - val_loss: 1.5626 - val_acc: 0.9602\n",
            "\n",
            "Epoch 00076: val_acc did not improve from 0.96372\n",
            "Epoch 77/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5544 - acc: 0.9498 - val_loss: 1.5624 - val_acc: 0.9639\n",
            "\n",
            "Epoch 00077: val_acc improved from 0.96372 to 0.96395, saving model to face_autoencoder.h5\n",
            "Epoch 78/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5545 - acc: 0.9500 - val_loss: 1.5628 - val_acc: 0.9404\n",
            "\n",
            "Epoch 00078: val_acc did not improve from 0.96395\n",
            "Epoch 79/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5544 - acc: 0.9504 - val_loss: 1.5628 - val_acc: 0.9219\n",
            "\n",
            "Epoch 00079: val_acc did not improve from 0.96395\n",
            "Epoch 80/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5544 - acc: 0.9491 - val_loss: 1.5632 - val_acc: 0.9520\n",
            "\n",
            "Epoch 00080: val_acc did not improve from 0.96395\n",
            "Epoch 81/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5544 - acc: 0.9505 - val_loss: 1.5620 - val_acc: 0.9473\n",
            "\n",
            "Epoch 00081: val_acc did not improve from 0.96395\n",
            "Epoch 82/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5543 - acc: 0.9503 - val_loss: 1.5623 - val_acc: 0.9416\n",
            "\n",
            "Epoch 00082: val_acc did not improve from 0.96395\n",
            "Epoch 83/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5542 - acc: 0.9509 - val_loss: 1.5622 - val_acc: 0.9630\n",
            "\n",
            "Epoch 00083: val_acc did not improve from 0.96395\n",
            "Epoch 84/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5543 - acc: 0.9508 - val_loss: 1.5626 - val_acc: 0.9449\n",
            "\n",
            "Epoch 00084: val_acc did not improve from 0.96395\n",
            "Epoch 85/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5543 - acc: 0.9508 - val_loss: 1.5626 - val_acc: 0.9606\n",
            "\n",
            "Epoch 00085: val_acc did not improve from 0.96395\n",
            "Epoch 86/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5545 - acc: 0.9497 - val_loss: 1.5832 - val_acc: 0.8164\n",
            "\n",
            "Epoch 00086: val_acc did not improve from 0.96395\n",
            "Epoch 87/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5549 - acc: 0.9480 - val_loss: 1.5659 - val_acc: 0.9202\n",
            "\n",
            "Epoch 00087: val_acc did not improve from 0.96395\n",
            "Epoch 88/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5545 - acc: 0.9505 - val_loss: 1.5629 - val_acc: 0.9460\n",
            "\n",
            "Epoch 00088: val_acc did not improve from 0.96395\n",
            "Epoch 89/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5543 - acc: 0.9501 - val_loss: 1.5632 - val_acc: 0.9483\n",
            "\n",
            "Epoch 00089: val_acc did not improve from 0.96395\n",
            "Epoch 90/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5543 - acc: 0.9501 - val_loss: 1.5655 - val_acc: 0.9302\n",
            "\n",
            "Epoch 00090: val_acc did not improve from 0.96395\n",
            "Epoch 91/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5542 - acc: 0.9502 - val_loss: 1.5625 - val_acc: 0.9297\n",
            "\n",
            "Epoch 00091: val_acc did not improve from 0.96395\n",
            "Epoch 92/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5541 - acc: 0.9513 - val_loss: 1.5625 - val_acc: 0.9396\n",
            "\n",
            "Epoch 00092: val_acc did not improve from 0.96395\n",
            "Epoch 93/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5541 - acc: 0.9516 - val_loss: 1.5624 - val_acc: 0.9624\n",
            "\n",
            "Epoch 00093: val_acc did not improve from 0.96395\n",
            "Epoch 94/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5541 - acc: 0.9517 - val_loss: 1.5624 - val_acc: 0.9376\n",
            "\n",
            "Epoch 00094: val_acc did not improve from 0.96395\n",
            "Epoch 95/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5540 - acc: 0.9511 - val_loss: 1.5621 - val_acc: 0.9653\n",
            "\n",
            "Epoch 00095: val_acc improved from 0.96395 to 0.96535, saving model to face_autoencoder.h5\n",
            "Epoch 96/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5540 - acc: 0.9514 - val_loss: 1.5625 - val_acc: 0.9263\n",
            "\n",
            "Epoch 00096: val_acc did not improve from 0.96535\n",
            "Epoch 97/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5540 - acc: 0.9516 - val_loss: 1.5624 - val_acc: 0.9496\n",
            "\n",
            "Epoch 00097: val_acc did not improve from 0.96535\n",
            "Epoch 98/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5542 - acc: 0.9502 - val_loss: 1.5625 - val_acc: 0.9408\n",
            "\n",
            "Epoch 00098: val_acc did not improve from 0.96535\n",
            "Epoch 99/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5543 - acc: 0.9507 - val_loss: 1.5628 - val_acc: 0.9624\n",
            "\n",
            "Epoch 00099: val_acc did not improve from 0.96535\n",
            "Epoch 100/100\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.5541 - acc: 0.9518 - val_loss: 1.5630 - val_acc: 0.9259\n",
            "\n",
            "Epoch 00100: val_acc did not improve from 0.96535\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Gits8TcFkRG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "c729ce6c-6a2d-49c4-8d26-c3df7ec1e6a9"
      },
      "source": [
        "loss = autoencoder_train.history['loss']\n",
        "val_loss = autoencoder_train.history['val_loss']\n",
        "epochs = range(100)\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label = 'Training Loss')\n",
        "plt.plot(epochs, val_loss, 'b', label = 'Validation Loss')\n",
        "plt.title('Training and Validation Loss for Autoencoder Faces Model')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEICAYAAABS0fM3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\no8AxITAMwyhw/j+xDHr6ez0aygAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlUQOLuqGDdT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "autoencoder.save_weights('Autoencoder_Faces_Weights.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCo4pjT6GSqA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_img = Input(shape = (64, 64, 3))\n",
        "x = Conv2D(32, (3,3), activation = 'relu', padding = 'same')(input_img)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(32, (3,3), activation = 'relu', padding = 'same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2,2), padding = 'same')(x)\n",
        "\n",
        "x = Conv2D(64, (3,3), activation = 'relu', padding = 'same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(64, (3,3), activation = 'relu', padding = 'same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2,2), padding = 'same')(x)\n",
        "\n",
        "x = Conv2D(128, (3,3), activation = 'relu', padding = 'same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(128, (3,3), activation = 'relu', padding = 'same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2,2), padding = 'same')(x)\n",
        "\n",
        "x = Conv2D(256, (3,3), activation = 'relu', padding = 'same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(256, (3,3), activation = 'relu', padding = 'same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2,2), padding = 'same')(x)\n",
        "\n",
        "x = Conv2D(512, (3,3), activation = 'relu', padding = 'same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(512, (3,3), activation = 'relu', padding = 'same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2,2), padding = 'same')(x)\n",
        "\n",
        "x = Flatten()(x)\n",
        "x = Dense(128, activation = 'relu')(x)\n",
        "output = Dense(160, activation = 'softmax')(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHUP0gmKGjWD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "full_model = Model(input_img, output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hbk5Qxa7Gr_a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cd75c664-bf02-45d8-fdbd-cf62656ce439"
      },
      "source": [
        "len(full_model.layers)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kKQhQ9UOaUz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for l1, l2 in zip(full_model.layers[:26], autoencoder.layers[0:26]):\n",
        "  l1.set_weights(l2.get_weights())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8jtDWBJOsYe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4c0be305-b06f-4367-cfca-48b3bed50f59"
      },
      "source": [
        "autoencoder.get_weights()[0][1]"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[-0.13205335, -0.09362765, -0.01680565, -0.04386548,\n",
              "          0.07735714, -0.11334502, -0.07388371, -0.10048779,\n",
              "         -0.03278591, -0.04950107, -0.11741651, -0.09982558,\n",
              "         -0.05605496,  0.05711193,  0.10572096,  0.01279089,\n",
              "          0.05406658,  0.17087547, -0.14371319, -0.17108881,\n",
              "         -0.04456146, -0.09664675, -0.03279164, -0.12719059,\n",
              "         -0.12144398,  0.08033599,  0.01919387, -0.0389156 ,\n",
              "          0.11676726,  0.00249902,  0.05575931, -0.09397209],\n",
              "        [ 0.14094172, -0.01029029,  0.02237911, -0.03719529,\n",
              "         -0.13880298,  0.15350087,  0.06964681,  0.06165554,\n",
              "         -0.08416193,  0.10543042, -0.06946059, -0.02356171,\n",
              "         -0.11121215,  0.1073991 , -0.01460132,  0.02247388,\n",
              "         -0.0888775 , -0.02956122,  0.11848701,  0.09147485,\n",
              "         -0.02630254, -0.08377942, -0.02699859,  0.04592564,\n",
              "          0.00887432, -0.00145939, -0.09636784,  0.03761072,\n",
              "          0.12377509,  0.025453  , -0.09374881, -0.05756369],\n",
              "        [-0.12921423,  0.01772832,  0.02646222,  0.09997018,\n",
              "         -0.05038928,  0.07211085, -0.22740343, -0.07813768,\n",
              "         -0.02537345,  0.06791748,  0.11224626,  0.04094896,\n",
              "         -0.0711529 , -0.06046744, -0.00307095, -0.08520128,\n",
              "         -0.11942668,  0.12658875,  0.00523348,  0.06426585,\n",
              "          0.01943443, -0.00565018, -0.01786437, -0.03555846,\n",
              "         -0.10002063,  0.06027343, -0.17950574,  0.03725369,\n",
              "         -0.04881087,  0.01909754, -0.08996093,  0.0401978 ]],\n",
              "\n",
              "       [[ 0.0193815 ,  0.05626787, -0.00921385, -0.1314007 ,\n",
              "          0.0682616 ,  0.05647175, -0.02897102, -0.07452135,\n",
              "         -0.06969136,  0.03222447, -0.21201012,  0.07250997,\n",
              "         -0.12412751,  0.08571348, -0.17023423,  0.10138941,\n",
              "          0.04243255,  0.15406501,  0.10143196,  0.04492036,\n",
              "          0.13680297,  0.01121611, -0.0047555 , -0.08178785,\n",
              "          0.00414117, -0.0105559 ,  0.03437811,  0.11183427,\n",
              "         -0.03739339,  0.03480688,  0.13002819,  0.0929336 ],\n",
              "        [ 0.07570211, -0.04337774, -0.04518161,  0.06459163,\n",
              "         -0.13627858,  0.02961803, -0.12108517,  0.12427182,\n",
              "          0.10328227,  0.02245581, -0.05473488, -0.08092034,\n",
              "         -0.13369003,  0.06860709,  0.00937294, -0.18628156,\n",
              "          0.10148378, -0.09267414,  0.10052262, -0.14206712,\n",
              "         -0.09504555, -0.00203549,  0.04350807, -0.06331433,\n",
              "         -0.02289447,  0.08282471, -0.06794012,  0.00867085,\n",
              "          0.0254095 , -0.01322179, -0.00987783, -0.07581514],\n",
              "        [-0.16680811, -0.00485498, -0.0725069 , -0.03668496,\n",
              "          0.08358581, -0.02614287,  0.00328205,  0.08040123,\n",
              "         -0.12881246, -0.03112285,  0.11859076,  0.0108146 ,\n",
              "          0.00094866,  0.10531337,  0.13335098,  0.13716505,\n",
              "          0.14236967,  0.14275958,  0.0306986 , -0.2027632 ,\n",
              "         -0.12883498, -0.11033965,  0.09760774, -0.10993281,\n",
              "         -0.1249188 , -0.10202182, -0.02008436,  0.03821601,\n",
              "         -0.06902621, -0.10711002, -0.06571396, -0.02738333]],\n",
              "\n",
              "       [[ 0.13190661, -0.16755152, -0.11924703, -0.07965473,\n",
              "          0.0043049 , -0.07285844, -0.16975395,  0.08380751,\n",
              "          0.00158847, -0.09403025, -0.28202823,  0.04209121,\n",
              "          0.02382753,  0.05406927, -0.11823022, -0.11020894,\n",
              "         -0.08541013,  0.12714386, -0.14134103, -0.12376044,\n",
              "          0.16699299,  0.00730587, -0.05709048, -0.03709116,\n",
              "          0.09520087,  0.08963291,  0.03657654, -0.0718969 ,\n",
              "         -0.10035307,  0.10331842,  0.18104017, -0.11340509],\n",
              "        [ 0.00412642,  0.03526943, -0.06839928,  0.0603895 ,\n",
              "         -0.0735475 ,  0.0643898 ,  0.08901834,  0.07244492,\n",
              "          0.07101136,  0.04613558, -0.12103466, -0.12851132,\n",
              "          0.03441638, -0.05002682, -0.0124424 , -0.06536866,\n",
              "         -0.05174933, -0.08538207,  0.0256893 , -0.05073824,\n",
              "         -0.12762322, -0.00453886,  0.02406148,  0.04646151,\n",
              "          0.08123112,  0.04570274, -0.00096401, -0.00668232,\n",
              "          0.08146043,  0.07712962, -0.03841978,  0.08130332],\n",
              "        [-0.03607495,  0.12314101, -0.07463835,  0.08197296,\n",
              "         -0.08661509,  0.10864192, -0.14624387, -0.08773415,\n",
              "          0.05443984,  0.08159544,  0.10191647, -0.03955129,\n",
              "         -0.08790666, -0.15061039, -0.06500787,  0.05008211,\n",
              "         -0.02845753,  0.10993614, -0.03948639,  0.11187625,\n",
              "          0.03114719,  0.14167064, -0.05625946, -0.0036158 ,\n",
              "         -0.11788259, -0.06510469,  0.07803108, -0.09041504,\n",
              "         -0.14057267, -0.13031714, -0.10903911, -0.12969318]]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QulZsGIqOwaC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "89c2f60d-3bd3-4cbe-d66f-bd22af3886c3"
      },
      "source": [
        "full_model.get_weights()[0][1]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[-0.13205335, -0.09362765, -0.01680565, -0.04386548,\n",
              "          0.07735714, -0.11334502, -0.07388371, -0.10048779,\n",
              "         -0.03278591, -0.04950107, -0.11741651, -0.09982558,\n",
              "         -0.05605496,  0.05711193,  0.10572096,  0.01279089,\n",
              "          0.05406658,  0.17087547, -0.14371319, -0.17108881,\n",
              "         -0.04456146, -0.09664675, -0.03279164, -0.12719059,\n",
              "         -0.12144398,  0.08033599,  0.01919387, -0.0389156 ,\n",
              "          0.11676726,  0.00249902,  0.05575931, -0.09397209],\n",
              "        [ 0.14094172, -0.01029029,  0.02237911, -0.03719529,\n",
              "         -0.13880298,  0.15350087,  0.06964681,  0.06165554,\n",
              "         -0.08416193,  0.10543042, -0.06946059, -0.02356171,\n",
              "         -0.11121215,  0.1073991 , -0.01460132,  0.02247388,\n",
              "         -0.0888775 , -0.02956122,  0.11848701,  0.09147485,\n",
              "         -0.02630254, -0.08377942, -0.02699859,  0.04592564,\n",
              "          0.00887432, -0.00145939, -0.09636784,  0.03761072,\n",
              "          0.12377509,  0.025453  , -0.09374881, -0.05756369],\n",
              "        [-0.12921423,  0.01772832,  0.02646222,  0.09997018,\n",
              "         -0.05038928,  0.07211085, -0.22740343, -0.07813768,\n",
              "         -0.02537345,  0.06791748,  0.11224626,  0.04094896,\n",
              "         -0.0711529 , -0.06046744, -0.00307095, -0.08520128,\n",
              "         -0.11942668,  0.12658875,  0.00523348,  0.06426585,\n",
              "          0.01943443, -0.00565018, -0.01786437, -0.03555846,\n",
              "         -0.10002063,  0.06027343, -0.17950574,  0.03725369,\n",
              "         -0.04881087,  0.01909754, -0.08996093,  0.0401978 ]],\n",
              "\n",
              "       [[ 0.0193815 ,  0.05626787, -0.00921385, -0.1314007 ,\n",
              "          0.0682616 ,  0.05647175, -0.02897102, -0.07452135,\n",
              "         -0.06969136,  0.03222447, -0.21201012,  0.07250997,\n",
              "         -0.12412751,  0.08571348, -0.17023423,  0.10138941,\n",
              "          0.04243255,  0.15406501,  0.10143196,  0.04492036,\n",
              "          0.13680297,  0.01121611, -0.0047555 , -0.08178785,\n",
              "          0.00414117, -0.0105559 ,  0.03437811,  0.11183427,\n",
              "         -0.03739339,  0.03480688,  0.13002819,  0.0929336 ],\n",
              "        [ 0.07570211, -0.04337774, -0.04518161,  0.06459163,\n",
              "         -0.13627858,  0.02961803, -0.12108517,  0.12427182,\n",
              "          0.10328227,  0.02245581, -0.05473488, -0.08092034,\n",
              "         -0.13369003,  0.06860709,  0.00937294, -0.18628156,\n",
              "          0.10148378, -0.09267414,  0.10052262, -0.14206712,\n",
              "         -0.09504555, -0.00203549,  0.04350807, -0.06331433,\n",
              "         -0.02289447,  0.08282471, -0.06794012,  0.00867085,\n",
              "          0.0254095 , -0.01322179, -0.00987783, -0.07581514],\n",
              "        [-0.16680811, -0.00485498, -0.0725069 , -0.03668496,\n",
              "          0.08358581, -0.02614287,  0.00328205,  0.08040123,\n",
              "         -0.12881246, -0.03112285,  0.11859076,  0.0108146 ,\n",
              "          0.00094866,  0.10531337,  0.13335098,  0.13716505,\n",
              "          0.14236967,  0.14275958,  0.0306986 , -0.2027632 ,\n",
              "         -0.12883498, -0.11033965,  0.09760774, -0.10993281,\n",
              "         -0.1249188 , -0.10202182, -0.02008436,  0.03821601,\n",
              "         -0.06902621, -0.10711002, -0.06571396, -0.02738333]],\n",
              "\n",
              "       [[ 0.13190661, -0.16755152, -0.11924703, -0.07965473,\n",
              "          0.0043049 , -0.07285844, -0.16975395,  0.08380751,\n",
              "          0.00158847, -0.09403025, -0.28202823,  0.04209121,\n",
              "          0.02382753,  0.05406927, -0.11823022, -0.11020894,\n",
              "         -0.08541013,  0.12714386, -0.14134103, -0.12376044,\n",
              "          0.16699299,  0.00730587, -0.05709048, -0.03709116,\n",
              "          0.09520087,  0.08963291,  0.03657654, -0.0718969 ,\n",
              "         -0.10035307,  0.10331842,  0.18104017, -0.11340509],\n",
              "        [ 0.00412642,  0.03526943, -0.06839928,  0.0603895 ,\n",
              "         -0.0735475 ,  0.0643898 ,  0.08901834,  0.07244492,\n",
              "          0.07101136,  0.04613558, -0.12103466, -0.12851132,\n",
              "          0.03441638, -0.05002682, -0.0124424 , -0.06536866,\n",
              "         -0.05174933, -0.08538207,  0.0256893 , -0.05073824,\n",
              "         -0.12762322, -0.00453886,  0.02406148,  0.04646151,\n",
              "          0.08123112,  0.04570274, -0.00096401, -0.00668232,\n",
              "          0.08146043,  0.07712962, -0.03841978,  0.08130332],\n",
              "        [-0.03607495,  0.12314101, -0.07463835,  0.08197296,\n",
              "         -0.08661509,  0.10864192, -0.14624387, -0.08773415,\n",
              "          0.05443984,  0.08159544,  0.10191647, -0.03955129,\n",
              "         -0.08790666, -0.15061039, -0.06500787,  0.05008211,\n",
              "         -0.02845753,  0.10993614, -0.03948639,  0.11187625,\n",
              "          0.03114719,  0.14167064, -0.05625946, -0.0036158 ,\n",
              "         -0.11788259, -0.06510469,  0.07803108, -0.09041504,\n",
              "         -0.14057267, -0.13031714, -0.10903911, -0.12969318]]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umKs2Qa7O4mC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for layer in full_model.layers[0:26]:\n",
        "  layer.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vrn8ALcO8CQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "full_model.compile(loss = 'categorical_crossentropy', metrics = ['accuracy'], optimizer = 'adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uic8dkv1PBsj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b96c74b4-7619-41fd-9ecb-3d5815a093fa"
      },
      "source": [
        "full_model.summary()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, 64, 64, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 64, 64, 32)        896       \n",
            "_________________________________________________________________\n",
            "batch_normalization_21 (Batc (None, 64, 64, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 64, 64, 32)        9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_22 (Batc (None, 64, 64, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_24 (Conv2D)           (None, 32, 32, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_23 (Batc (None, 32, 32, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_25 (Conv2D)           (None, 32, 32, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_24 (Batc (None, 32, 32, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_26 (Conv2D)           (None, 16, 16, 128)       73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_25 (Batc (None, 16, 16, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_27 (Conv2D)           (None, 16, 16, 128)       147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_26 (Batc (None, 16, 16, 128)       512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_28 (Conv2D)           (None, 8, 8, 256)         295168    \n",
            "_________________________________________________________________\n",
            "batch_normalization_27 (Batc (None, 8, 8, 256)         1024      \n",
            "_________________________________________________________________\n",
            "conv2d_29 (Conv2D)           (None, 8, 8, 256)         590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_28 (Batc (None, 8, 8, 256)         1024      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_30 (Conv2D)           (None, 4, 4, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "batch_normalization_29 (Batc (None, 4, 4, 512)         2048      \n",
            "_________________________________________________________________\n",
            "conv2d_31 (Conv2D)           (None, 4, 4, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "batch_normalization_30 (Batc (None, 4, 4, 512)         2048      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               262272    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 160)               20640     \n",
            "=================================================================\n",
            "Total params: 5,003,072\n",
            "Trainable params: 282,912\n",
            "Non-trainable params: 4,720,160\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaSjjcXuPD4J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2fb93cff-a160-4d2d-8e1f-051373e2c725"
      },
      "source": [
        "filepath = 'pretrained_face_best_weights.h5'\n",
        "checkpoint = ModelCheckpoint(filepath, monitor = 'val_acc', verbose = 1, save_best_only = True, mode = 'max')\n",
        "callbacks_list = [checkpoint]\n",
        "pretrained_classifier_train = full_model.fit(X_train, y_train, epochs = 100, batch_size = 256, verbose = 1, callbacks = callbacks_list, validation_data = [X_val, y_val])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 24000 samples, validate on 7994 samples\n",
            "Epoch 1/100\n",
            "24000/24000 [==============================] - 11s 462us/step - loss: 4.7028 - acc: 0.0591 - val_loss: 3.9413 - val_acc: 0.1566\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.15662, saving model to pretrained_face_best_weights.h5\n",
            "Epoch 2/100\n",
            "24000/24000 [==============================] - 10s 416us/step - loss: 3.1899 - acc: 0.2768 - val_loss: 2.7404 - val_acc: 0.3459\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.15662 to 0.34588, saving model to pretrained_face_best_weights.h5\n",
            "Epoch 3/100\n",
            "24000/24000 [==============================] - 10s 419us/step - loss: 2.2909 - acc: 0.4450 - val_loss: 2.2334 - val_acc: 0.4463\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.34588 to 0.44633, saving model to pretrained_face_best_weights.h5\n",
            "Epoch 4/100\n",
            "24000/24000 [==============================] - 10s 415us/step - loss: 1.7969 - acc: 0.5585 - val_loss: 1.8286 - val_acc: 0.5425\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.44633 to 0.54253, saving model to pretrained_face_best_weights.h5\n",
            "Epoch 5/100\n",
            "24000/24000 [==============================] - 10s 416us/step - loss: 1.5024 - acc: 0.6252 - val_loss: 1.6462 - val_acc: 0.5844\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.54253 to 0.58444, saving model to pretrained_face_best_weights.h5\n",
            "Epoch 6/100\n",
            "24000/24000 [==============================] - 10s 416us/step - loss: 1.2886 - acc: 0.6821 - val_loss: 1.4716 - val_acc: 0.6328\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.58444 to 0.63285, saving model to pretrained_face_best_weights.h5\n",
            "Epoch 7/100\n",
            "24000/24000 [==============================] - 10s 417us/step - loss: 1.1229 - acc: 0.7229 - val_loss: 1.3623 - val_acc: 0.6546\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.63285 to 0.65462, saving model to pretrained_face_best_weights.h5\n",
            "Epoch 8/100\n",
            "24000/24000 [==============================] - 10s 418us/step - loss: 0.9990 - acc: 0.7548 - val_loss: 1.2580 - val_acc: 0.6815\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.65462 to 0.68151, saving model to pretrained_face_best_weights.h5\n",
            "Epoch 9/100\n",
            "24000/24000 [==============================] - 10s 418us/step - loss: 0.9003 - acc: 0.7788 - val_loss: 1.2051 - val_acc: 0.6971\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.68151 to 0.69715, saving model to pretrained_face_best_weights.h5\n",
            "Epoch 10/100\n",
            "24000/24000 [==============================] - 10s 417us/step - loss: 0.8280 - acc: 0.7952 - val_loss: 1.1660 - val_acc: 0.7030\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.69715 to 0.70303, saving model to pretrained_face_best_weights.h5\n",
            "Epoch 11/100\n",
            "24000/24000 [==============================] - 10s 417us/step - loss: 0.7484 - acc: 0.8185 - val_loss: 1.1265 - val_acc: 0.7125\n",
            "\n",
            "Epoch 00011: val_acc improved from 0.70303 to 0.71253, saving model to pretrained_face_best_weights.h5\n",
            "Epoch 12/100\n",
            "24000/24000 [==============================] - 10s 417us/step - loss: 0.6968 - acc: 0.8314 - val_loss: 1.0555 - val_acc: 0.7282\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.71253 to 0.72817, saving model to pretrained_face_best_weights.h5\n",
            "Epoch 13/100\n",
            "24000/24000 [==============================] - 10s 416us/step - loss: 0.6455 - acc: 0.8413 - val_loss: 1.0207 - val_acc: 0.7401\n",
            "\n",
            "Epoch 00013: val_acc improved from 0.72817 to 0.74006, saving model to pretrained_face_best_weights.h5\n",
            "Epoch 14/100\n",
            "24000/24000 [==============================] - 10s 418us/step - loss: 0.6010 - acc: 0.8535 - val_loss: 1.0296 - val_acc: 0.7433\n",
            "\n",
            "Epoch 00014: val_acc improved from 0.74006 to 0.74331, saving model to pretrained_face_best_weights.h5\n",
            "Epoch 15/100\n",
            "24000/24000 [==============================] - 10s 418us/step - loss: 0.5511 - acc: 0.8660 - val_loss: 0.9530 - val_acc: 0.7588\n",
            "\n",
            "Epoch 00015: val_acc improved from 0.74331 to 0.75882, saving model to pretrained_face_best_weights.h5\n",
            "Epoch 16/100\n",
            "24000/24000 [==============================] - 10s 417us/step - loss: 0.5148 - acc: 0.8737 - val_loss: 0.9521 - val_acc: 0.7589\n",
            "\n",
            "Epoch 00016: val_acc improved from 0.75882 to 0.75894, saving model to pretrained_face_best_weights.h5\n",
            "Epoch 17/100\n",
            "24000/24000 [==============================] - 10s 417us/step - loss: 0.4792 - acc: 0.8860 - val_loss: 0.9167 - val_acc: 0.7743\n",
            "\n",
            "Epoch 00017: val_acc improved from 0.75894 to 0.77433, saving model to pretrained_face_best_weights.h5\n",
            "Epoch 18/100\n",
            "24000/24000 [==============================] - 10s 416us/step - loss: 0.4500 - acc: 0.8927 - val_loss: 0.9469 - val_acc: 0.7619\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.77433\n",
            "Epoch 19/100\n",
            "24000/24000 [==============================] - 10s 419us/step - loss: 0.4281 - acc: 0.8945 - val_loss: 0.9237 - val_acc: 0.7721\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.77433\n",
            "Epoch 20/100\n",
            "24000/24000 [==============================] - 10s 416us/step - loss: 0.4000 - acc: 0.9038 - val_loss: 0.8646 - val_acc: 0.7905\n",
            "\n",
            "Epoch 00020: val_acc improved from 0.77433 to 0.79047, saving model to pretrained_face_best_weights.h5\n",
            "Epoch 21/100\n",
            "24000/24000 [==============================] - 10s 416us/step - loss: 0.3735 - acc: 0.9116 - val_loss: 0.8623 - val_acc: 0.7842\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.79047\n",
            "Epoch 22/100\n",
            "24000/24000 [==============================] - 10s 418us/step - loss: 0.3469 - acc: 0.9179 - val_loss: 0.8818 - val_acc: 0.7793\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.79047\n",
            "Epoch 23/100\n",
            "24000/24000 [==============================] - 10s 418us/step - loss: 0.3346 - acc: 0.9197 - val_loss: 0.8177 - val_acc: 0.7990\n",
            "\n",
            "Epoch 00023: val_acc improved from 0.79047 to 0.79897, saving model to pretrained_face_best_weights.h5\n",
            "Epoch 24/100\n",
            "24000/24000 [==============================] - 10s 419us/step - loss: 0.3159 - acc: 0.9245 - val_loss: 0.8841 - val_acc: 0.7843\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.79897\n",
            "Epoch 25/100\n",
            "24000/24000 [==============================] - 10s 419us/step - loss: 0.2930 - acc: 0.9302 - val_loss: 0.8189 - val_acc: 0.8010\n",
            "\n",
            "Epoch 00025: val_acc improved from 0.79897 to 0.80098, saving model to pretrained_face_best_weights.h5\n",
            "Epoch 26/100\n",
            "24000/24000 [==============================] - 10s 416us/step - loss: 0.2784 - acc: 0.9353 - val_loss: 0.8052 - val_acc: 0.8035\n",
            "\n",
            "Epoch 00026: val_acc improved from 0.80098 to 0.80348, saving model to pretrained_face_best_weights.h5\n",
            "Epoch 27/100\n",
            "24000/24000 [==============================] - 10s 417us/step - loss: 0.2711 - acc: 0.9363 - val_loss: 0.8260 - val_acc: 0.8011\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.80348\n",
            "Epoch 28/100\n",
            "24000/24000 [==============================] - 10s 420us/step - loss: 0.2452 - acc: 0.9435 - val_loss: 0.7758 - val_acc: 0.8106\n",
            "\n",
            "Epoch 00028: val_acc improved from 0.80348 to 0.81061, saving model to pretrained_face_best_weights.h5\n",
            "Epoch 29/100\n",
            "24000/24000 [==============================] - 10s 417us/step - loss: 0.2326 - acc: 0.9468 - val_loss: 0.7807 - val_acc: 0.8126\n",
            "\n",
            "Epoch 00029: val_acc improved from 0.81061 to 0.81261, saving model to pretrained_face_best_weights.h5\n",
            "Epoch 30/100\n",
            "24000/24000 [==============================] - 10s 418us/step - loss: 0.2196 - acc: 0.9492 - val_loss: 0.8218 - val_acc: 0.8050\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.81261\n",
            "Epoch 31/100\n",
            "24000/24000 [==============================] - 10s 419us/step - loss: 0.2088 - acc: 0.9538 - val_loss: 0.7830 - val_acc: 0.8196\n",
            "\n",
            "Epoch 00031: val_acc improved from 0.81261 to 0.81961, saving model to pretrained_face_best_weights.h5\n",
            "Epoch 32/100\n",
            "24000/24000 [==============================] - 10s 418us/step - loss: 0.1997 - acc: 0.9545 - val_loss: 0.8234 - val_acc: 0.8076\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.81961\n",
            "Epoch 33/100\n",
            "24000/24000 [==============================] - 10s 417us/step - loss: 0.1834 - acc: 0.9604 - val_loss: 0.7922 - val_acc: 0.8141\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.81961\n",
            "Epoch 34/100\n",
            "24000/24000 [==============================] - 10s 417us/step - loss: 0.1757 - acc: 0.9614 - val_loss: 0.7817 - val_acc: 0.8170\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.81961\n",
            "Epoch 35/100\n",
            "24000/24000 [==============================] - 10s 419us/step - loss: 0.1636 - acc: 0.9640 - val_loss: 0.8401 - val_acc: 0.8094\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.81961\n",
            "Epoch 36/100\n",
            "24000/24000 [==============================] - 10s 418us/step - loss: 0.1644 - acc: 0.9643 - val_loss: 0.8300 - val_acc: 0.8075\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.81961\n",
            "Epoch 37/100\n",
            "24000/24000 [==============================] - 10s 417us/step - loss: 0.1458 - acc: 0.9706 - val_loss: 0.7717 - val_acc: 0.8236\n",
            "\n",
            "Epoch 00037: val_acc improved from 0.81961 to 0.82362, saving model to pretrained_face_best_weights.h5\n",
            "Epoch 38/100\n",
            "24000/24000 [==============================] - 10s 417us/step - loss: 0.1419 - acc: 0.9710 - val_loss: 0.7992 - val_acc: 0.8186\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.82362\n",
            "Epoch 39/100\n",
            "24000/24000 [==============================] - 10s 417us/step - loss: 0.1346 - acc: 0.9726 - val_loss: 0.7717 - val_acc: 0.8267\n",
            "\n",
            "Epoch 00039: val_acc improved from 0.82362 to 0.82675, saving model to pretrained_face_best_weights.h5\n",
            "Epoch 40/100\n",
            "24000/24000 [==============================] - 10s 418us/step - loss: 0.1215 - acc: 0.9770 - val_loss: 0.7870 - val_acc: 0.8222\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.82675\n",
            "Epoch 41/100\n",
            "24000/24000 [==============================] - 10s 418us/step - loss: 0.1152 - acc: 0.9787 - val_loss: 0.8278 - val_acc: 0.8166\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.82675\n",
            "Epoch 42/100\n",
            "24000/24000 [==============================] - 10s 417us/step - loss: 0.1078 - acc: 0.9802 - val_loss: 0.8316 - val_acc: 0.8202\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.82675\n",
            "Epoch 43/100\n",
            "24000/24000 [==============================] - 10s 417us/step - loss: 0.1075 - acc: 0.9796 - val_loss: 0.7695 - val_acc: 0.8301\n",
            "\n",
            "Epoch 00043: val_acc improved from 0.82675 to 0.83012, saving model to pretrained_face_best_weights.h5\n",
            "Epoch 44/100\n",
            "24000/24000 [==============================] - 10s 417us/step - loss: 0.0998 - acc: 0.9825 - val_loss: 0.8055 - val_acc: 0.8222\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.83012\n",
            "Epoch 45/100\n",
            "24000/24000 [==============================] - 10s 418us/step - loss: 0.0956 - acc: 0.9823 - val_loss: 0.8108 - val_acc: 0.8255\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.83012\n",
            "Epoch 46/100\n",
            "24000/24000 [==============================] - 10s 417us/step - loss: 0.0889 - acc: 0.9851 - val_loss: 0.7863 - val_acc: 0.8327\n",
            "\n",
            "Epoch 00046: val_acc improved from 0.83012 to 0.83275, saving model to pretrained_face_best_weights.h5\n",
            "Epoch 47/100\n",
            "24000/24000 [==============================] - 10s 415us/step - loss: 0.0875 - acc: 0.9843 - val_loss: 0.8444 - val_acc: 0.8181\n",
            "\n",
            "Epoch 00047: val_acc did not improve from 0.83275\n",
            "Epoch 48/100\n",
            "24000/24000 [==============================] - 10s 414us/step - loss: 0.0827 - acc: 0.9860 - val_loss: 0.8599 - val_acc: 0.8170\n",
            "\n",
            "Epoch 00048: val_acc did not improve from 0.83275\n",
            "Epoch 49/100\n",
            "24000/24000 [==============================] - 10s 416us/step - loss: 0.0756 - acc: 0.9879 - val_loss: 0.8226 - val_acc: 0.8280\n",
            "\n",
            "Epoch 00049: val_acc did not improve from 0.83275\n",
            "Epoch 50/100\n",
            "24000/24000 [==============================] - 10s 418us/step - loss: 0.0758 - acc: 0.9872 - val_loss: 0.8279 - val_acc: 0.8265\n",
            "\n",
            "Epoch 00050: val_acc did not improve from 0.83275\n",
            "Epoch 51/100\n",
            "24000/24000 [==============================] - 10s 418us/step - loss: 0.0696 - acc: 0.9888 - val_loss: 0.8584 - val_acc: 0.8211\n",
            "\n",
            "Epoch 00051: val_acc did not improve from 0.83275\n",
            "Epoch 52/100\n",
            "24000/24000 [==============================] - 10s 420us/step - loss: 0.0657 - acc: 0.9899 - val_loss: 0.8402 - val_acc: 0.8260\n",
            "\n",
            "Epoch 00052: val_acc did not improve from 0.83275\n",
            "Epoch 53/100\n",
            "24000/24000 [==============================] - 10s 417us/step - loss: 0.0631 - acc: 0.9897 - val_loss: 0.8421 - val_acc: 0.8212\n",
            "\n",
            "Epoch 00053: val_acc did not improve from 0.83275\n",
            "Epoch 54/100\n",
            "24000/24000 [==============================] - 10s 418us/step - loss: 0.0601 - acc: 0.9910 - val_loss: 0.8295 - val_acc: 0.8327\n",
            "\n",
            "Epoch 00054: val_acc improved from 0.83275 to 0.83275, saving model to pretrained_face_best_weights.h5\n",
            "Epoch 55/100\n",
            "24000/24000 [==============================] - 10s 413us/step - loss: 0.0602 - acc: 0.9908 - val_loss: 0.8461 - val_acc: 0.8301\n",
            "\n",
            "Epoch 00055: val_acc did not improve from 0.83275\n",
            "Epoch 56/100\n",
            "24000/24000 [==============================] - 10s 414us/step - loss: 0.0572 - acc: 0.9915 - val_loss: 0.8545 - val_acc: 0.8256\n",
            "\n",
            "Epoch 00056: val_acc did not improve from 0.83275\n",
            "Epoch 57/100\n",
            "24000/24000 [==============================] - 10s 414us/step - loss: 0.0517 - acc: 0.9930 - val_loss: 0.8530 - val_acc: 0.8281\n",
            "\n",
            "Epoch 00057: val_acc did not improve from 0.83275\n",
            "Epoch 58/100\n",
            "24000/24000 [==============================] - 10s 417us/step - loss: 0.0505 - acc: 0.9928 - val_loss: 0.8244 - val_acc: 0.8364\n",
            "\n",
            "Epoch 00058: val_acc improved from 0.83275 to 0.83638, saving model to pretrained_face_best_weights.h5\n",
            "Epoch 59/100\n",
            "24000/24000 [==============================] - 10s 414us/step - loss: 0.0490 - acc: 0.9926 - val_loss: 0.8712 - val_acc: 0.8227\n",
            "\n",
            "Epoch 00059: val_acc did not improve from 0.83638\n",
            "Epoch 60/100\n",
            "24000/24000 [==============================] - 10s 412us/step - loss: 0.0482 - acc: 0.9932 - val_loss: 0.8469 - val_acc: 0.8360\n",
            "\n",
            "Epoch 00060: val_acc did not improve from 0.83638\n",
            "Epoch 61/100\n",
            "24000/24000 [==============================] - 10s 414us/step - loss: 0.0522 - acc: 0.9908 - val_loss: 0.8675 - val_acc: 0.8295\n",
            "\n",
            "Epoch 00061: val_acc did not improve from 0.83638\n",
            "Epoch 62/100\n",
            "24000/24000 [==============================] - 10s 415us/step - loss: 0.0539 - acc: 0.9896 - val_loss: 0.8389 - val_acc: 0.8312\n",
            "\n",
            "Epoch 00062: val_acc did not improve from 0.83638\n",
            "Epoch 63/100\n",
            "24000/24000 [==============================] - 10s 417us/step - loss: 0.0531 - acc: 0.9903 - val_loss: 0.9040 - val_acc: 0.8232\n",
            "\n",
            "Epoch 00063: val_acc did not improve from 0.83638\n",
            "Epoch 64/100\n",
            "24000/24000 [==============================] - 10s 415us/step - loss: 0.0483 - acc: 0.9919 - val_loss: 0.8612 - val_acc: 0.8344\n",
            "\n",
            "Epoch 00064: val_acc did not improve from 0.83638\n",
            "Epoch 65/100\n",
            "24000/24000 [==============================] - 10s 414us/step - loss: 0.0441 - acc: 0.9934 - val_loss: 0.8874 - val_acc: 0.8266\n",
            "\n",
            "Epoch 00065: val_acc did not improve from 0.83638\n",
            "Epoch 66/100\n",
            "24000/24000 [==============================] - 10s 415us/step - loss: 0.0439 - acc: 0.9924 - val_loss: 0.8529 - val_acc: 0.8366\n",
            "\n",
            "Epoch 00066: val_acc improved from 0.83638 to 0.83663, saving model to pretrained_face_best_weights.h5\n",
            "Epoch 67/100\n",
            "24000/24000 [==============================] - 10s 414us/step - loss: 0.0447 - acc: 0.9925 - val_loss: 0.8975 - val_acc: 0.8269\n",
            "\n",
            "Epoch 00067: val_acc did not improve from 0.83663\n",
            "Epoch 68/100\n",
            "24000/24000 [==============================] - 10s 419us/step - loss: 0.0344 - acc: 0.9951 - val_loss: 0.8786 - val_acc: 0.8326\n",
            "\n",
            "Epoch 00068: val_acc did not improve from 0.83663\n",
            "Epoch 69/100\n",
            "24000/24000 [==============================] - 10s 417us/step - loss: 0.0328 - acc: 0.9954 - val_loss: 0.8826 - val_acc: 0.8327\n",
            "\n",
            "Epoch 00069: val_acc did not improve from 0.83663\n",
            "Epoch 70/100\n",
            "24000/24000 [==============================] - 10s 418us/step - loss: 0.0314 - acc: 0.9958 - val_loss: 0.8900 - val_acc: 0.8307\n",
            "\n",
            "Epoch 00070: val_acc did not improve from 0.83663\n",
            "Epoch 71/100\n",
            "24000/24000 [==============================] - 10s 417us/step - loss: 0.0333 - acc: 0.9953 - val_loss: 0.9378 - val_acc: 0.8215\n",
            "\n",
            "Epoch 00071: val_acc did not improve from 0.83663\n",
            "Epoch 72/100\n",
            "24000/24000 [==============================] - 10s 416us/step - loss: 0.0435 - acc: 0.9915 - val_loss: 0.9348 - val_acc: 0.8232\n",
            "\n",
            "Epoch 00072: val_acc did not improve from 0.83663\n",
            "Epoch 73/100\n",
            "24000/24000 [==============================] - 10s 417us/step - loss: 0.0553 - acc: 0.9869 - val_loss: 0.9192 - val_acc: 0.8262\n",
            "\n",
            "Epoch 00073: val_acc did not improve from 0.83663\n",
            "Epoch 74/100\n",
            "24000/24000 [==============================] - 10s 416us/step - loss: 0.0678 - acc: 0.9818 - val_loss: 0.9365 - val_acc: 0.8272\n",
            "\n",
            "Epoch 00074: val_acc did not improve from 0.83663\n",
            "Epoch 75/100\n",
            "24000/24000 [==============================] - 10s 415us/step - loss: 0.0557 - acc: 0.9860 - val_loss: 0.9371 - val_acc: 0.8255\n",
            "\n",
            "Epoch 00075: val_acc did not improve from 0.83663\n",
            "Epoch 76/100\n",
            "24000/24000 [==============================] - 10s 416us/step - loss: 0.0329 - acc: 0.9943 - val_loss: 0.8961 - val_acc: 0.8374\n",
            "\n",
            "Epoch 00076: val_acc improved from 0.83663 to 0.83738, saving model to pretrained_face_best_weights.h5\n",
            "Epoch 77/100\n",
            "24000/24000 [==============================] - 10s 413us/step - loss: 0.0222 - acc: 0.9975 - val_loss: 0.8549 - val_acc: 0.8431\n",
            "\n",
            "Epoch 00077: val_acc improved from 0.83738 to 0.84313, saving model to pretrained_face_best_weights.h5\n",
            "Epoch 78/100\n",
            "24000/24000 [==============================] - 10s 415us/step - loss: 0.0143 - acc: 0.9995 - val_loss: 0.8428 - val_acc: 0.8499\n",
            "\n",
            "Epoch 00078: val_acc improved from 0.84313 to 0.84989, saving model to pretrained_face_best_weights.h5\n",
            "Epoch 79/100\n",
            "24000/24000 [==============================] - 10s 413us/step - loss: 0.0130 - acc: 0.9995 - val_loss: 0.8820 - val_acc: 0.8379\n",
            "\n",
            "Epoch 00079: val_acc did not improve from 0.84989\n",
            "Epoch 80/100\n",
            "24000/24000 [==============================] - 10s 419us/step - loss: 0.0162 - acc: 0.9989 - val_loss: 0.8902 - val_acc: 0.8431\n",
            "\n",
            "Epoch 00080: val_acc did not improve from 0.84989\n",
            "Epoch 81/100\n",
            "24000/24000 [==============================] - 10s 415us/step - loss: 0.0127 - acc: 0.9996 - val_loss: 0.8707 - val_acc: 0.8444\n",
            "\n",
            "Epoch 00081: val_acc did not improve from 0.84989\n",
            "Epoch 82/100\n",
            "24000/24000 [==============================] - 10s 413us/step - loss: 0.0122 - acc: 0.9995 - val_loss: 0.8805 - val_acc: 0.8426\n",
            "\n",
            "Epoch 00082: val_acc did not improve from 0.84989\n",
            "Epoch 83/100\n",
            "24000/24000 [==============================] - 10s 414us/step - loss: 0.0137 - acc: 0.9995 - val_loss: 0.8819 - val_acc: 0.8438\n",
            "\n",
            "Epoch 00083: val_acc did not improve from 0.84989\n",
            "Epoch 84/100\n",
            "24000/24000 [==============================] - 10s 414us/step - loss: 0.0122 - acc: 0.9997 - val_loss: 0.8461 - val_acc: 0.8518\n",
            "\n",
            "Epoch 00084: val_acc improved from 0.84989 to 0.85176, saving model to pretrained_face_best_weights.h5\n",
            "Epoch 85/100\n",
            "24000/24000 [==============================] - 10s 416us/step - loss: 0.0119 - acc: 0.9996 - val_loss: 0.8650 - val_acc: 0.8490\n",
            "\n",
            "Epoch 00085: val_acc did not improve from 0.85176\n",
            "Epoch 86/100\n",
            "24000/24000 [==============================] - 10s 415us/step - loss: 0.0143 - acc: 0.9990 - val_loss: 0.9024 - val_acc: 0.8418\n",
            "\n",
            "Epoch 00086: val_acc did not improve from 0.85176\n",
            "Epoch 87/100\n",
            "24000/24000 [==============================] - 10s 415us/step - loss: 0.0449 - acc: 0.9892 - val_loss: 1.1487 - val_acc: 0.7923\n",
            "\n",
            "Epoch 00087: val_acc did not improve from 0.85176\n",
            "Epoch 88/100\n",
            "24000/24000 [==============================] - 10s 416us/step - loss: 0.1697 - acc: 0.9438 - val_loss: 1.0478 - val_acc: 0.8140\n",
            "\n",
            "Epoch 00088: val_acc did not improve from 0.85176\n",
            "Epoch 89/100\n",
            "24000/24000 [==============================] - 10s 414us/step - loss: 0.1190 - acc: 0.9627 - val_loss: 1.2080 - val_acc: 0.7870\n",
            "\n",
            "Epoch 00089: val_acc did not improve from 0.85176\n",
            "Epoch 90/100\n",
            "24000/24000 [==============================] - 10s 415us/step - loss: 0.0745 - acc: 0.9771 - val_loss: 0.9227 - val_acc: 0.8388\n",
            "\n",
            "Epoch 00090: val_acc did not improve from 0.85176\n",
            "Epoch 91/100\n",
            "24000/24000 [==============================] - 10s 418us/step - loss: 0.0250 - acc: 0.9955 - val_loss: 0.8979 - val_acc: 0.8466\n",
            "\n",
            "Epoch 00091: val_acc did not improve from 0.85176\n",
            "Epoch 92/100\n",
            "24000/24000 [==============================] - 10s 414us/step - loss: 0.0128 - acc: 0.9989 - val_loss: 0.8700 - val_acc: 0.8526\n",
            "\n",
            "Epoch 00092: val_acc improved from 0.85176 to 0.85264, saving model to pretrained_face_best_weights.h5\n",
            "Epoch 93/100\n",
            "24000/24000 [==============================] - 10s 417us/step - loss: 0.0086 - acc: 0.9997 - val_loss: 0.8594 - val_acc: 0.8544\n",
            "\n",
            "Epoch 00093: val_acc improved from 0.85264 to 0.85439, saving model to pretrained_face_best_weights.h5\n",
            "Epoch 94/100\n",
            "24000/24000 [==============================] - 10s 417us/step - loss: 0.0084 - acc: 0.9996 - val_loss: 0.8783 - val_acc: 0.8518\n",
            "\n",
            "Epoch 00094: val_acc did not improve from 0.85439\n",
            "Epoch 95/100\n",
            "24000/24000 [==============================] - 10s 417us/step - loss: 0.0070 - acc: 0.9998 - val_loss: 0.8839 - val_acc: 0.8509\n",
            "\n",
            "Epoch 00095: val_acc did not improve from 0.85439\n",
            "Epoch 96/100\n",
            "24000/24000 [==============================] - 10s 414us/step - loss: 0.0080 - acc: 0.9995 - val_loss: 0.8917 - val_acc: 0.8484\n",
            "\n",
            "Epoch 00096: val_acc did not improve from 0.85439\n",
            "Epoch 97/100\n",
            "24000/24000 [==============================] - 10s 416us/step - loss: 0.0071 - acc: 0.9998 - val_loss: 0.8774 - val_acc: 0.8530\n",
            "\n",
            "Epoch 00097: val_acc did not improve from 0.85439\n",
            "Epoch 98/100\n",
            "24000/24000 [==============================] - 10s 417us/step - loss: 0.0065 - acc: 0.9998 - val_loss: 0.8688 - val_acc: 0.8558\n",
            "\n",
            "Epoch 00098: val_acc improved from 0.85439 to 0.85577, saving model to pretrained_face_best_weights.h5\n",
            "Epoch 99/100\n",
            "24000/24000 [==============================] - 10s 417us/step - loss: 0.0061 - acc: 0.9998 - val_loss: 0.8804 - val_acc: 0.8505\n",
            "\n",
            "Epoch 00099: val_acc did not improve from 0.85577\n",
            "Epoch 100/100\n",
            "24000/24000 [==============================] - 10s 417us/step - loss: 0.0058 - acc: 0.9998 - val_loss: 0.8768 - val_acc: 0.8564\n",
            "\n",
            "Epoch 00100: val_acc improved from 0.85577 to 0.85639, saving model to pretrained_face_best_weights.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euJ4zgp-Paf7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "full_model.save_weights('full_model_pretrained_faces_weights.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sAisql_P0Z9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "outputId": "be1792b0-19b5-452f-bb22-864b506dfe5e"
      },
      "source": [
        "accuracy = pretrained_classifier_train.history['acc']\n",
        "val_accuracy = pretrained_classifier_train.history['val_acc']\n",
        "loss = pretrained_classifier_train.history['loss']\n",
        "val_loss = pretrained_classifier_train.history['val_loss']\n",
        "epochs = range(len(accuracy))\n",
        "plt.plot(epochs, accuracy, 'bo', label = 'Training Accuracy')\n",
        "plt.plot(epochs, val_accuracy, 'b', label = 'Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy using Pretrained Autoencoder Weights')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label = 'Training Loss')\n",
        "plt.plot(epochs, val_loss, 'b', label = 'Validation Loss')\n",
        "plt.title('Training and Validation Loss using Pretrained Autoencoder Weights')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcIAAAEICAYAAADMa/SXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaYAAAEICAYAAAAUZ1CdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ns3gSEdqmAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h468PJm5P67y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_eval = full_model.evaluate(X_test, y_test, verbose = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sv9BtyfYQTiS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9e200e4c-d343-4075-cd37-d1b471aef311"
      },
      "source": [
        "print('Test loss : ', test_eval[0])\n",
        "print('Test accuracy : ', test_eval[1])"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss :  1.0425702388510107\n",
            "Test accuracy :  0.82975\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYEuqqWgQZGB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_classes = full_model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYcDQQJWQh4P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_classes = np.argmax(np.round(predicted_classes), axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibsVenxYXuyk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_classes = lb.inverse_transform(predicted_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoQPyjDlQkba",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_df['predicted_classes'] = predicted_classes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gd4Aj5cWQ8vl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "1ab2fc72-1643-44f9-bc26-e41fe29499c2"
      },
      "source": [
        "test_df.head()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>test_labels</th>\n",
              "      <th>predicted_classes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Sergey_Lavrov</td>\n",
              "      <td>Sergey_Lavrov</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Gloria_Macapagal_Arroyo</td>\n",
              "      <td>Gloria_Macapagal_Arroyo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>George_W_Bush</td>\n",
              "      <td>George_W_Bush</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Michael_Bloomberg</td>\n",
              "      <td>Michael_Bloomberg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Nestor_Kirchner</td>\n",
              "      <td>Tommy_Franks</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                test_labels         predicted_classes\n",
              "0            Sergey_Lavrov             Sergey_Lavrov \n",
              "1  Gloria_Macapagal_Arroyo   Gloria_Macapagal_Arroyo \n",
              "2            George_W_Bush             George_W_Bush \n",
              "3        Michael_Bloomberg         Michael_Bloomberg \n",
              "4          Nestor_Kirchner              Tommy_Franks "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "361MEaf9RCWw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "0f9e7bea-8c0a-4105-f561-2eda758b581a"
      },
      "source": [
        "test_df[test_df['test_labels'] != test_df['predicted_classes']]"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>test_labels</th>\n",
              "      <th>predicted_classes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Nestor_Kirchner</td>\n",
              "      <td>Tommy_Franks</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Trent_Lott</td>\n",
              "      <td>Bill_McBride</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Pierce_Brosnan</td>\n",
              "      <td>Juan_Carlos_Ferrero</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>John_Snow</td>\n",
              "      <td>Dick_Cheney</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>Atal_Bihari_Vajpayee</td>\n",
              "      <td>Alejandro_Toledo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7984</th>\n",
              "      <td>Muhammad_Ali</td>\n",
              "      <td>Dominique_de_Villepin</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7986</th>\n",
              "      <td>Pervez_Musharraf</td>\n",
              "      <td>Silvio_Berlusconi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7994</th>\n",
              "      <td>Lance_Armstrong</td>\n",
              "      <td>Rudolph_Giuliani</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7996</th>\n",
              "      <td>Vaclav_Havel</td>\n",
              "      <td>Ricardo_Lagos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7998</th>\n",
              "      <td>Richard_Gephardt</td>\n",
              "      <td>Alejandro_Toledo</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1409 rows  2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                test_labels       predicted_classes\n",
              "4          Nestor_Kirchner            Tommy_Franks \n",
              "12              Trent_Lott            Bill_McBride \n",
              "16          Pierce_Brosnan     Juan_Carlos_Ferrero \n",
              "32               John_Snow             Dick_Cheney \n",
              "34    Atal_Bihari_Vajpayee        Alejandro_Toledo \n",
              "...                     ...                     ...\n",
              "7984          Muhammad_Ali   Dominique_de_Villepin \n",
              "7986      Pervez_Musharraf       Silvio_Berlusconi \n",
              "7994       Lance_Armstrong        Rudolph_Giuliani \n",
              "7996          Vaclav_Havel           Ricardo_Lagos \n",
              "7998      Richard_Gephardt        Alejandro_Toledo \n",
              "\n",
              "[1409 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Awt3rhQ-RdxR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "03d63cb4-2bcd-4822-9833-caeecc2de972"
      },
      "source": [
        "1-round(len(test_df[test_df['test_labels'] != test_df['predicted_classes']])/len(test_df),3)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8240000000000001"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5i4hBL0Rusf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_df.to_csv('predicted_results.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sj9Z1bqaYIl9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}